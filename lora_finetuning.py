# -*- coding: utf-8 -*-
"""LoRa_FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d1rd6iMbN5dUTxWiHsBzuLSDAWtXe1RS
"""

!pip install transformers peft datasets

!pip install --upgrade datasets huggingface-hub fsspec

from transformers import AutoTokenizer,DataCollatorForLanguageModeling ,TextDataset , AutoModelForCausalLM , pipeline , Trainer , TrainingArguments
from datasets import load_dataset
from peft import get_peft_model , LoraConfig,TaskType
import os

from google.colab import files
uploaded=files.upload()

import os
os.getcwd()
os.listdir()

model_name='gpt2'
tokenizer=AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token=tokenizer.eos_token
tokenizer.pad_token_id=tokenizer.eos_token_id
dataset=load_dataset('text',data_files='/content/Akhenaten.txt')
def tokenizer_function(example):
  return tokenizer(example['text'],
                   padding='max_length',
                   truncation=True,
                   max_length=128)
tokenized_dataset=dataset.map(tokenizer_function,batched=True)

data_collator=DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

model=AutoModelForCausalLM.from_pretrained('gpt2-large')

lora_config=LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=.1,
    target_modules=['c_attn','c_proj']
)

model =get_peft_model(model, lora_config)
model.print_trainable_parameters()

training_args=TrainingArguments(
    output_dir='./results',
    eval_strategy='epoch',
    save_strategy='epoch',
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=10,
    learning_rate=1e-4,
    weight_decay=.01,
    load_best_model_at_end=True,
    report_to='none'
)
trainer2=Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator     ,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['train'],

)
trainer2.train()

model.save_pretrained('lora_gpt2')

from peft import PeftModel
base_model=AutoModelForCausalLM.from_pretrained('gpt2-large')
lora_model=PeftModel.from_pretrained(base_model,'lora_gpt2')

tokenizer=AutoTokenizer.from_pretrained('gpt2-large')
input_txt='akhenaton was a'
input=tokenizer(input_txt,return_tensors='pt')
output=lora_model.generate(**input,max_length=50)
print(tokenizer.decode(output[0],skip_special_tokens=True))